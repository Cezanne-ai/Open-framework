{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Input processing I - dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA2DkwUPPC6M"
      },
      "source": [
        "Input/text processing needs to be implemented at two different levels:\n",
        "\n",
        "Main level. The objective is not to eliminate/stem/lemma too much data/info, not to lose precious information. Also, the chronological steps are important to be implemented gradually, for different algorithms to use the current state of the processed text/input. The sub-words tokenization can be a solution but depending on the language can affect the model. The same with stop-words or keywords that can impact our objective to have a multi-domain conversational bot.\n",
        "\n",
        "Local/Algorithm level. More elaborate processing depending on the objectives.\n",
        "The first processing needs to be focusing on better understanding of the language and conversational specificities. At the same time, we need to separate the information transmitted by the user in five categories:\n",
        "\n",
        "•\tWords, numerical data and characters not needed in the main NIU model, for GDPR reasons or due to model limitations to understand them (ex: visual inputs or links that are in scope of future developments).\n",
        "\n",
        "•\tNumerical data needed especially for task-oriented user requests (ex: reservation for 3 persons on Monday, 5th January). As an exception, specific words need to be transmitted to DER (Data Entity Recognition, we will talk more later).\n",
        "\n",
        "•\tEmoji that will be analyzed separately.\n",
        "\n",
        "•\tWords in the core-input that will be processed.\n",
        "\n",
        "•\tPunctuation together with other information that must be analyzed in context.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caTnQEGfPrWQ"
      },
      "source": [
        "# IN: corrected list with words and characters (pipeline-Corrected words)\n",
        "# OUT: processed input sent to diffrent pipelines for computation : EER , DER, Grammar-Semantics, CVM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-frKrxDBPW9q"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tLinguistic evaluation. Addressing special characters of the language. (For example: removing hyphens by separating into 2 words; \n",
        "\n",
        "•\tRemoving words/ characters not needed; \n",
        "\n",
        "•\tAddressing numerical data, punctuation, emoji;\n",
        "\n",
        "Language specificities: yes (check language specificities);\n",
        "\n",
        "Dependencies: DER/CVM/EER/Grammar-Semantics;\n",
        "\n",
        "Database/ Vocabularies needed: Lexicon/ Dictionaries for numerical data and special dates & events /Emoji.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE1xSUZyO7ZH"
      },
      "source": [
        "# To dos:\n",
        "# 1.\tAddressing special characters of the language in order for all words/characters to be included into a list that can be processed further (we will leave this task for linguists to assess).\n",
        "# 2.\tAll numerical characters found (including in a special vocabulary) are marked for DER.\n",
        "# 3.\tEmoji are marked and sent to EER.\n",
        "# 4.\tMark nonexistent vocabulary words with UNK.\n",
        "# 5.\tMark words that exist more than once in vocabularies for Grammar/Semantics analysis.\n",
        "# 6.\tRemove personal names, telephone numbers, visual inputs, links, other characters like $#@ and the related words.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtApPKAGUiPr"
      },
      "source": [
        "Ude existing codes\n",
        "see code example!!! do not use before adapting to objectives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLdXFeSVUlZX"
      },
      "source": [
        "\n",
        "# import re                                  # library for regular expression operations\n",
        "# import string                              # for string operations\n",
        "\n",
        "# from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "# from nltk.stem import PorterStemmer        # module for stemming\n",
        "# from nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n",
        "\n",
        "# remove old style retweet text \"RT\"\n",
        "# tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "# remove hyperlinks\n",
        "# tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
        "\n",
        "# remove hashtags\n",
        "# only removing the hash # sign from the word\n",
        "# tweet2 = re.sub(r'#', '', tweet2)\n",
        "\n",
        "# instantiate tokenizer class\n",
        "# tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "\n",
        "# tokenize tweets\n",
        "# tweet_tokens = tokenizer.tokenize(tweet2)\n",
        "\n",
        "#Import the english stop words list from NLTK\n",
        "# stopwords_english = stopwords.words('english')\n",
        "\n",
        "# tweets_clean = []\n",
        "\n",
        "# for word in tweet_tokens: # Go through every word in your tokens list\n",
        "   # if (word not in stopwords_english and  # remove stopwords\n",
        "      #  word not in string.punctuation):  # remove punctuation\n",
        "       # tweets_clean.append(word)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utbRRqhXTElR"
      },
      "source": [
        "# def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    \"\"\"\n",
        "    Replace words not in the given vocabulary with '<unk>' token.\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of strings\n",
        "        vocabulary: List of strings that we will use\n",
        "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of strings, with words not in the vocabulary replaced\n",
        "    \"\"\"\n",
        "    \n",
        "    # Place vocabulary into a set for faster search\n",
        "    vocabulary = set(vocabulary)\n",
        "    \n",
        "    # Initialize a list that will hold the sentences\n",
        "    # after less frequent words are replaced by the unknown token\n",
        "    replaced_tokenized_sentences = []\n",
        "    \n",
        "    # Go through each sentence\n",
        "    for sentence in tokenized_sentences:\n",
        "        \n",
        "        # Initialize the list that will contain\n",
        "        # a single sentence with \"unknown_token\" replacements\n",
        "        replaced_sentence = []\n",
        "\n",
        "        # for each token in the sentence\n",
        "        for token in sentence: # complete this line\n",
        "            \n",
        "            # Check if the token is in the closed vocabulary\n",
        "            if token in vocabulary: # complete this line\n",
        "                # If so, append the word to the replaced_sentence\n",
        "                replaced_sentence.append(token)\n",
        "            else:\n",
        "                # otherwise, append the unknown token instead\n",
        "                replaced_sentence.append(unknown_token)\n",
        "        \n",
        "        # Append the list of tokens to the list of lists\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "        \n",
        "    return replaced_tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr_pBnC3XEVZ"
      },
      "source": [
        "# def tokenize(corpus):\n",
        "    data = re.sub(r'[,!?;-]+', '.', corpus)\n",
        "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
        "    data = [ ch.lower() for ch in data\n",
        "             if ch.isalpha()\n",
        "             or ch == '.'\n",
        "             or emoji.get_emoji_regexp().search(ch)\n",
        "           ]\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXvkAsodXGcA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}