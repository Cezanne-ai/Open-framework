{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8. Splitting sentences dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRVub6f3rzrK"
      },
      "source": [
        "“Divide et impera” is one of the strategies of our Pirkin 1 model. In this strategy, we will split the input (that can be a very elaborate description or can contain both sentence-intents and reactions) in short sentences.\n",
        "\n",
        "At the same time, in case of the same consecutive POS separated by space, comma, “or”, or “and” we will keep only one of the POS and all the others will be sent to Input Processing 2. From our experience, also the existing language platforms have issues with this kind of situations, that are poorly administered by language models, especially the bi-directional ones (like BERT.\n",
        "\n",
        "In the example: ‘I do not want to go home or to the cinema’, the masking model could easily miss that “not” is referring to the cinema also. \n",
        "\n",
        "There are NLP solutions like sentence-based tokenizers or libraries with sentence recognizers that can be useful..\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxBnGW6ArtdP"
      },
      "source": [
        "# IN: input from Grammar semantics and punctuation processing\n",
        "# OUT: sent data to Input processing 2 and Embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWq8NFSxsDo_"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tSplitting the input in sentences;\n",
        "\n",
        "•\tIdentifying consecutive words with the same POS (except verbs) that are separated by space, \"or”, “and”, “comma“;\n",
        "\n",
        "Language specificities: yes, for choosing which consecutive POS to use in the main model; \n",
        "\n",
        "Dependencies: Grammar-Semantics/Input Processing II & Embeddings;\n",
        "\n",
        "Database/ Vocabularies needed: specific vocabularies; \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4LkIZmcsLbn"
      },
      "source": [
        "# To dos:\n",
        "# 1.\tExtract important verbs from specific vocabularies by using frequencies.\n",
        "# 2.\tWhen the following punctuation is identified, separate in different sentences: “.”,”:”,”(“,”!”,”?”.\n",
        "# 3.\tIf 2 verbs are consecutive, we keep the one in the database \"important verb” or the second one. The other is eliminated.\n",
        "# 4.\tIf the POSs are consecutive or separated by space, \"or” “and” “comma” we will keep the first word in the string and the others will be sent to Input Processing 2 & Embeddings.\n",
        "# 5.\tThe sentence before “but” is eliminated.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNmpFD-HstV4"
      },
      "source": [
        "use existing codes\n",
        "\n",
        "adapt the following to the objectives:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Ircpg3s0ii"
      },
      "source": [
        "# import math\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import nltk\n",
        "# nltk.data.path.append('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DZwNWo7tNYY"
      },
      "source": [
        "# def split_to_sentences(data):\n",
        "    \"\"\"\n",
        "    Split data by linebreak \"\\n\"\n",
        "    \n",
        "    Args:\n",
        "        data: str\n",
        "    \n",
        "    Returns:\n",
        "        A list of sentences\n",
        "    \"\"\"\n",
        "    \n",
        "    sentences = data.split('\\n')\n",
        "    \n",
        "    \n",
        "    # Additional clearning (This part is already implemented)\n",
        "    # - Remove leading and trailing spaces from each sentence\n",
        "    # - Drop sentences if they are empty strings.\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    \n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRCXomOttYag"
      },
      "source": [
        "# def tokenize_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Tokenize sentences into tokens (words)\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of strings\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the list of lists of tokenized sentences\n",
        "    tokenized_sentences = []\n",
        "    \n",
        "    # Go through each sentence\n",
        "    for sentence in sentences:\n",
        "        \n",
        "        # Convert to lowercase letters\n",
        "        sentence = sentence.lower()\n",
        "        \n",
        "        # Convert into a list of words\n",
        "        tokenized = nltk.word_tokenize(sentence)\n",
        "        \n",
        "        # append the list of words to the list of lists\n",
        "        tokenized_sentences.append(tokenized)\n",
        "    \n",
        "    \n",
        "    return tokenized_sentences"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}