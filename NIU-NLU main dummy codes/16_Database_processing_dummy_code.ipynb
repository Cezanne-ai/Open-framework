{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16. Database processing dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMuObOuZWlLR"
      },
      "source": [
        "In reality, a person understands you based on past conversations that he had with other people (this is the fundamental principle of labeling and it is right). Therefore, familiar words are the first that stand out when trying to understand the other person’s utterance. For this reason, we will process the limited database (with interactions in the same environment and going through the same pipelines, as we are doing for inputs) in order to give the bot some prioritization and additional data for word embedding. Using additional resources like the internet or other similar conversation is helpful as you want to cover more topics/wording, but in the end can affect the understanding, as a different environment or grounding gives different meaning to words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4u7c3XAWRb3"
      },
      "source": [
        "#IN: databases corpuses\n",
        "#OUT: processed database, vocabularies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwM64w5UW3Ve"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tCreating vocabulary with word frequencies for each database; \n",
        "\n",
        "•\tAnalyzing word POSs in inputs vs in databases; \n",
        "\n",
        "Dependencies: Input processing I & II / Composed words/ Grammar Semantics;\n",
        "\n",
        "Database/ Vocabularies needed: Database with chitchats/replies + NOG-NLG Labeled datasets + NIU-NLU Specific datasets \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaS3buDMXFQI"
      },
      "source": [
        "# To dos:\n",
        "# 1.\tEach database is processed in the same way the input is processed, without the creation of entities and matrices.\n",
        "# 2.\t4 steps in processing and the order: Input processing I, Composed words, Grammar-Semantics, Input processing II. \n",
        "# 3.\tAfter processing, apply word frequencies (depending on the POS) and create vocabularies for each database. \n",
        "# 4.\tAssign NER1, NER2…+ determine core-domain.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNZfOkvFXOLY"
      },
      "source": [
        "use existing algorithms + algorithms used in the pipelines Input processing I & II / Composed words/ Grammar Semantics.\n",
        "\n",
        "see additional codes below, but use them only after adapting to objectives!!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gucRXCT3XgwX"
      },
      "source": [
        "# def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "\n",
        "    Args:\n",
        "        Q1 (list): List of transformed (to tensor) questions.\n",
        "        Q2 (list): List of transformed (to tensor) questions.\n",
        "        batch_size (int): Number of elements per batch.\n",
        "        pad (int, optional): Pad character from the vocab. Defaults to 1.\n",
        "        shuffle (bool, optional): If the batches should be randomnized or not. Defaults to True.\n",
        "    Yields:\n",
        "        tuple: Of the form (input1, input2) with types (numpy.ndarray, numpy.ndarray)\n",
        "        NOTE: input1: inputs to your model [q1a, q2a, q3a, ...] i.e. (q1a,q1b) are duplicates\n",
        "              input2: targets to your model [q1b, q2b,q3b, ...] i.e. (q1a,q2i) i!=a are not duplicates\n",
        "    \"\"\"\n",
        "\n",
        "    input1 = []\n",
        "    input2 = []\n",
        "    idx = 0\n",
        "    len_q = len(Q1)\n",
        "    question_indexes = [*range(len_q)]\n",
        "    \n",
        "    if shuffle:\n",
        "        rnd.shuffle(question_indexes)\n",
        "    \n",
        "    while True:\n",
        "        if idx >= len_q:\n",
        "            # if idx is greater than or equal to len_q, set idx accordingly \n",
        "            # (Hint: look at the instructions above)\n",
        "            idx = len_q\n",
        "            # shuffle to get random batches if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(question_indexes)\n",
        "        \n",
        "        # get questions at the `question_indexes[idx]` position in Q1 and Q2\n",
        "        q1 = Q1[question_indexes[idx]]\n",
        "        q2 = Q2[question_indexes[idx]]\n",
        "        \n",
        "        # increment idx by 1\n",
        "        idx += 1\n",
        "        # append q1\n",
        "        input1.append(q1)\n",
        "        # append q2\n",
        "        input2.append(q2)\n",
        "        if len(input1) == batch_size:\n",
        "            # determine max_len as the longest question in input1 & input 2\n",
        "            # Hint: use the `max` function. \n",
        "            # take max of input1 & input2 and then max out of the two of them.\n",
        "            max_len = max(max([len(q) for q in input1]),max([len(q) for q in input2]))\n",
        "            # pad to power-of-2 (Hint: look at the instructions above)\n",
        "            max_len = 2**int(np.ceil(np.log2(max_len)))\n",
        "            b1 = []\n",
        "            b2 = []\n",
        "            for q1, q2 in zip(input1, input2):\n",
        "                # add [pad] to q1 until it reaches max_len\n",
        "                q1 = q1 + [pad] * (max_len - len(q1))\n",
        "                # add [pad] to q2 until it reaches max_len\n",
        "                q2 = q2 + [pad] * (max_len - len(q2))\n",
        "                # append q1\n",
        "                b1.append(q1)\n",
        "                # append q2\n",
        "                b2.append(q2)\n",
        "            # use b1 and b2\n",
        "            yield np.array(b1), np.array(b2)\n",
        "            # reset the batches\n",
        "            input1, input2 = [], []  # reset the batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbqz8MfdYCz3"
      },
      "source": [
        "# imports\n",
        "# import re # regular expression library; for tokenization of words\n",
        "# from collections import Counter # collections library; counter: dict subclass for counting hashable objects\n",
        "# import matplotlib.pyplot as plt # for data visualization\n",
        "\n",
        "# convert all letters to lower case\n",
        "# text_lowercase = text.lower()\n",
        "\n",
        "# some regex to tokenize the string to words and return them in a list\n",
        "# words = re.findall(r'\\w+', text_lowercase)\n",
        "\n",
        "# create vocab\n",
        "# vocab = set(words)\n",
        "\n",
        "# create vocab including word count\n",
        "# counts_a = dict()\n",
        "# for w in words:\n",
        "    counts_a[w] = counts_a.get(w,0)+1\n",
        "\n",
        "# create vocab including word count using collections.Counter\n",
        "# counts_b = dict()\n",
        "# counts_b = Counter(words)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}