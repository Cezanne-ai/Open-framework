{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "23. Back-up dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVF_lxaVVkfu"
      },
      "source": [
        "The three models that we will implement as back-up models  using the same datasets as for SPCA model:\n",
        "\n",
        "•\tEncoders-decoders model for chatbot task (aka. T5).\n",
        "\n",
        "•\tOpen domain dialogue model with summarization memory-augmentation (aka. PARL.AI).\n",
        "\n",
        "•\tOpen-Source Language Understanding with Dialogue Management (aka. RASA).\n",
        "\n",
        "As we dismissed the solutions offered by data augmentation methods and we are working in a very limited environment in terms of pre-trained models, emerging languages and datasets/ corpuses, using E2E models, even the most recent and advanced, would not give us satisfaction. But there are some arguments to use them anyway, besides the fact that they will give us a clear picture of our framework efficacity:\n",
        "\n",
        "•\tImplementing these models after a clean-up process (the pipelines from Machine Education and 4 pipelines in Machine Learning) will have a beneficial impact and diminish the need for huge datasets.\n",
        "\n",
        "•\tUsing three models instead of one increases the chances for success.\n",
        "\n",
        "•\tWe will use the validation given by the three matrices (CVM, AVM and IVM) also for the back-up models.\n",
        "\n",
        "Steps:\n",
        "\n",
        "•\tUse the core-input as it is, before the Input-processing II, stemming/lemma processes. \n",
        "\n",
        "•\tVerify if the back-up model was initiated by pipelines in CPL or NOG.\n",
        "\n",
        "•\tCheck if one of the back-up models (the order is the one presented above) is providing answers to the current core-inputs.\n",
        "\n",
        "•\tApply in the reset pipeline (from CPL) different methods to accommodate long-term memory: like summarization-augmentation, retrieval-augmentation or goldfish memory.\n",
        "\n",
        "•\tValidate the possible answers provided by the back-up model inside the CPL layer, by using the CVM/IVM/AVM.\n",
        "\n",
        "•\tAdapt these E2E models in order to be suited to the big architecture of our framework.  \n",
        "\n",
        "This layer is not intended for deep conversational inputs that were classified like that by domain validation pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyHdI9EzVdxK"
      },
      "source": [
        "# IN: same input as SPCA\n",
        "# OUT: a possible answer,correlated with memory update and CPL, in NOG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4N6j9jTV-kt"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tEvaluating the input of the user utterance in a E2E model;\n",
        "\n",
        "•\tUsing this model as a backup in case the user has negative sentiments about the Pirkin model answers; \n",
        "\n",
        "•\tUsing also this model if the bot automatically determines that it is more efficient;\n",
        "\n",
        "Dependencies: NOG/AVM/Auto-training/Splitting sentences/Memory update;\n",
        "\n",
        "Database/ Vocabularies needed: NIU-NLU Specific datasets, NOG-NLG Labeled datasets, Deep conversational datasets + corpuses from pre-trained models (if the case)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeNOM881WGsk"
      },
      "source": [
        "# To Dos:\n",
        "# 1.\tVerify if there is a request for a back up answer.\n",
        "# 2.\tApply one of the 3 models (Encoders-decoders model for chatbot task (aka. T5), Open domain dialogue model with summarization memory-augmentation (aka. PARL.AI), Open-Source Language Understanding with Dialogue Management (aka. RASA)) on the user input (the same input used also for SPCA, and the 3 models will be implemented in the order that were presented).\n",
        "# 3.\tDetermine what kind of backup is used depending on the database (back-up 1 for core-domain, back up 4 for deep conversational through book queries).\n",
        "# 4.\tBack up 1 will have 3 subdomains (Back up 1.1 for forms; for example: reservation, back up 1.2 if it is referring to NER1, and back-up 1 for the rest)\n",
        "# 5.\tUpdating IVM/AVM and go through the memory update pipeline for summarization-augmentation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPLM7oXdWUVc"
      },
      "source": [
        "use existing codes; see back-up repository that will be updated"
      ]
    }
  ]
}