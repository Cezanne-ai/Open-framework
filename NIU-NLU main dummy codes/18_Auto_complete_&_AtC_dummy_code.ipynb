{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "18. Auto complete & AtC dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWbQsPxW6_a6"
      },
      "source": [
        "An important part in a conversation is the intuition (other types of intuitions will be used in CPL and NOG). Even if some words are not explicit (many times the subject is implicit in some languages, for example), the other person can easily understand the utterance in context and finding the implicit or missing words can be more effective than a past utterance’s summarization when we try to fix the long-term memory. An algorithm that tries to discover the implicit words is also useful for the model, as statistically we will also deal with UNK words at this point. \n",
        "\n",
        "Labeling can solve this problem because machines can be trained to have some deviations (for example 80% understanding of the intent can be considered safe). We believe, though, that an NLP- DL algorithm for auto-complete & augmentation-through-conversation will do a better job because we will direct the training upon a specific job/task, not to mention that we will have a solution for goldfish memory.\n",
        "\n",
        "We will be using the bi-directional masking task from BERT with the existing labeled and unsupervised data (having books processed and trained in the model, suited for the domain in question, can be very effective for language model instead of pre-trained models with huge uncontrolled datasets). In order to accommodate our needs and subtract specific context- long-term memory-words especially for the SPCA pipelines, that will come next in our architecture, we will apply masking in 4 directions (in the order presented) on each user conversational turn that implies core-inputs:\n",
        "\n",
        "•\tWe will mask the first token of the utterances that do not have a subject/NER to see if we find the implicit Subject. The extracted word will be searched in the past utterances and if found, will become the new subject. In the next pipelines, we will have additional back-up methods for this task.\n",
        "\n",
        "•\tWe will mask the UNK words in terms of both vocabulary and POS and apply the same training flow. \n",
        "\n",
        "•\tWe will mask the last token and apply the same flow. GPT3 can be more effective for this task and then we will apply the same training flow.\n",
        "\n",
        "•\tWe will mask the last verb of the core-input. If after masking task we will have the same verb (or similar) we will consider this verb as the Predicate, if not we will also search in the past utterances applying the same training flow from the above tasks.\n",
        "\n",
        "This is the part of the solution in which we use neural networks, but we will also apply other methods for the same tasks in the following pipelines. Having limited resources and the datasets, books and the input already processed will have as result shorter training times, however this point needs to be tested. \n",
        "Input processing II will be repeated, if necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTJ8H_P16xdJ"
      },
      "source": [
        "# IN: sentences from splitting sentences with UNK or implicit words\n",
        "# OUT: enhanced sentences using aut-ocomplete and masking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi7U_hIA7UQC"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tWe will apply masking to find implicit words\n",
        "\n",
        "•\tUsing auto-complete in case a verb, an adjective or a noun is missing, and at the same time IVM/AVM indicates that there are no past intents interactions, in order to find the most probable words in the database, so as to complete the user input with implicit words.\n",
        "\n",
        "Dependencies: IVM/AVM/Input Processing II;\n",
        "\n",
        "Database/ Vocabularies needed: Database with chitchats/replies + NIU-NLU Specific datasets + NOG-NLG Labeled datasets + Deep conversational datasets;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH_hgPfs7bBA"
      },
      "source": [
        "# To dos:\n",
        "# Auto-Complete if there aren’t any past utterances:\n",
        "# Step 1. Search for UNK words. If found, skip step 2. In case of more UNK words, mark the last one.\n",
        "# Step 2. If we don’t have past inputs from the user, search if we have all these three POSs in the input in this order: verb, noun, adjective. Mark the POS (only one) that doesn't exist.\n",
        "# Step 3. Auto-complete the marked position/UNK word using the limited databases.\n",
        "# * Attention to the NER (ex: we will not do auto-complete for NER)\n",
        "# NLP masking task if there are past utterances:\n",
        "# A.\tWe will mask the first token of the utterances that do not have a subject/NER to see if we find the implicit Subject. The resulted word will be searched in the past utterances and if found, will become the new subject. \n",
        "# B.\tWe will mask the UNK words in terms of both vocabulary and POS and apply the same training flow. \n",
        "# C.\tWe will mask the last token and apply the same flow. GPT3 can be more effective for this task and then we will apply the same training flow.\n",
        "# D.\tWe will mask the last verb of the core-input. If after masking task we will have the same verb (or similar) we will consider this verb as the Predicate, if not we will also search in the past utterances applying the same training flow from the above tasks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3R83Y7h7uDB"
      },
      "source": [
        "use existing  for auto-complete tasks and BERT masking task\n",
        "\n",
        "for auto-complete see the following code as an example. Do not implement w/t adapting to objectives (also N-grams can be used if we have computational issues):\n",
        "\n",
        "https://github.com/amanjeetsahu/Natural-Language-Processing-Specialization/blob/master/Natural%20Language%20Processing%20with%20Probabilistic%20Models/Week%203/C2_W3_Assignment_Solution.ipynb"
      ]
    }
  ]
}