{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19. Embeddings dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTBMiCrO-BLc"
      },
      "source": [
        "Now, we need to transmit data to the bot for computation and understanding. We already transmitted important info through matrices and entities and we must transmit the core input (which is at this moment the remaining corpus of the input). As we don’t have a huge database and we created this detailed/unique architecture, it’s only natural to make our own word embedded vector & rules. \n",
        "Let’s say we want to describe the pointer finger functions. A solution is to mask the finger and then reconstruct it based on similarities with what we have in the datasets (this is pretty much the current state of the art solution). Another solution is to show the finger, independently, or in a relation with the previous finger, like RNN's. We are going to refer to Gregory Bateson's work to better understand how humans do it. A word doesn’t have only intrinsic or extrinsic representation, and it’s meaning can be extracted due to the relations with other words (the same is with the pointer finger). This solution limits the size of corpuses needed to build an accurate embedding if used in a sentence-intent model that already extracted the important meaning in terms of words and their syntactic functions.\n",
        "\n",
        "A word has 4 dimensions in terms of understanding:\n",
        "\n",
        "•\tIts intrinsic meaning; \n",
        "\n",
        "•\tIts meaning for other words; \n",
        "\n",
        "•\tIts meaning inside the sentence/phrase/description;\n",
        "\n",
        "•\tIts meaning for the persons/bot to whom you conversate.\n",
        "\n",
        "Consequently, in our model, a word will be vectorized through a special word vector  that covers the relations between the words in the entire input as fundamental for the real meaning of the word. The same vectorization process will be applied on the database and books. Doing this process will allow us to apply simple similarity functions between embedded word vectors from the input and the ones from the databases in order to extract/generate the answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSVkOGYD91RP"
      },
      "source": [
        "# IN: clean-up input/ core input + databases\n",
        "# OUT: embedding vector + k-word vector for every word in the input and databases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtMJsCIz-kp4"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tAnalyzing words that are used to empower other words;\n",
        "\n",
        "•\tFinding the most familiar word (for example a synonym to the word from the user that is more known) - we include here also words that are coming from Splitting sentences and composed words;\n",
        "\n",
        "•\tMaking a word embedding vector to cover all 4 dimensions of the word;\n",
        "Language specificities: yes for words connectors (ex: ''at\", “in”) and specific forms of the verbs (ex: “will be\", “would”); \n",
        "\n",
        "Dependencies: Splitting sentences/Composed words/ Input processing ll/Database processing/Books processing;\n",
        "\n",
        "Database/ Vocabularies needed: vocabularies for synonyms, regional words, neologisms: vocabulary with connectors;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9OUypFW-0JZ"
      },
      "source": [
        "# To dos:\n",
        "# 1.\tAssess the syntactic functions of words using connectors (ex : \"at”, “in”) + other types (like “will be”) + negations; then eliminate these words.\n",
        "# 2.\tBuilt a matrix (we will name it k-word) that has on the first column the following:\n",
        "# •\tIndex no. of the word in the Lexicon/specific vocabularies/additional index for the composed words;\n",
        "# •\tDomain identification; \n",
        "# •\tNER identification – will be regarded as slot types in training;\n",
        "# •\tThe POS of the word;\n",
        "# •\tFrequency of the word in the databases. NER will not have frequencies.\n",
        "# 3.\tInclude in the k-word additional columns: synonyms/ neologisms, regional words and words from splitting sentences.\n",
        "# 4.\tThe column with the highest frequency or with a slot type will become the k-word vector of the word.\n",
        "# 5.\tBuild the embedding vector of the word with the following data range between -3 and 3: frequency in the databases, frequency of the word in the input (if the words repeat in the same input), position of the word inside the input, position of the word vs the verbs/nouns/adjectives, category of the adjectives (from composed words).\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf4ay-D6_IN4"
      },
      "source": [
        "build code from scratch"
      ]
    }
  ]
}