{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "17. Books processing dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfAUuna1bJRK"
      },
      "source": [
        "In order to be used for NIU and NOG, all the books that are uploaded in the bot knowledge need to be first processed (going through the same process as the input ) because some data is irrelevant and others must be structured. Keep in mind that the model allows for future adding of books and the bot needs automatically to use the added knowledge in the understanding and in the generation processes. This layer is pretty much automatically transforming books in databases (unstructured data into structured) but keeping the interdependencies and the chronological order chosen by the author. \n",
        "\n",
        "Other significant factors:\n",
        "\n",
        "•\tThe type of book is important. The model will be able to work with three types: novels, scripts & screenplays (even if scripts are not actually books) and scientific books (all types – psychology and philosophy included).\n",
        "\n",
        "•\tThe names of the book characters in the novels and scripts are not important in the conversation because we don’t need the actual action from the book, but ideas. Deep conversation is not intended to replace the actual reading, but to have intelligent discussions based on some topics and promote the book reading.\n",
        "\n",
        "•\tAs we don’t have sufficient databases to do training it’s important to find a way to translate the book into a database as a strategic task.\n",
        "\n",
        "•\tFor the answer to be correlated with the user utterance, rows 2 and 3 of the answers CVMs need to correspond to the CVM of the input. For example, if the user is talking about the past, the searched answer should be in past tense. If the user is addressing in the first person, then the bot should reply in the second person, and vice versa.  \n",
        "\n",
        "A possibility is to do the same with site corpuses, meaning to process, train and then use them as databases without the need of API integration, if information from these sites is needed. But, as Bateson suggested, the arbitrary division can be misleading if we don’t apply scientific methods. Books have a prerequisite defined structure, they have either literary or scientific value, and we can use these fundamentals to walk through books’ labyrinths. On the opposite, sites are heterogeneous in terms of the structures and have a value depending on the users and products/services.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b1DbUvIbVhn"
      },
      "source": [
        "# IN: books corpuses\n",
        "# OUT: create additional databases and vocabularies for deep conversational topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBvQKdXRbaam"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tCreating vocabulary with word frequencies for each book; \n",
        "\n",
        "•\tTransforming the content of the books into a structured database that can be trained;\n",
        "\n",
        "Dependencies: NOG from books/Embeddings/ CVM/ Grammar/ Auto-correct;\n",
        "\n",
        "Database/ Vocabularies needed:  Deep conversational datasets;\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONENxpSvZceS"
      },
      "source": [
        "# To dos:\n",
        "# 1.\tAll books will be divided in chapters/sub-chapters/paragraphs/sentences, maintaining at the same time their initial place by using a special origin notation for NOG chronological correlations.\n",
        "# 2.\tApply Machine Education on every sentence (Grammar, NER and CVM).\n",
        "# 3.\tDelete paragraphs with NERs for novels and scripts.\n",
        "# 4.\tCVM matrix will be computed for each sentence. If a paragraph has more sentences, the CVM of the last sentence will be considered the one corresponding to the paragraph.\n",
        "# 5.\tAll bold paragraphs, chapters and sub-chapters names will be considered Book Intent Database (BID), together with all paragraphs that are considered questions by CVM matrix.\n",
        "# 6.\tAll the CVMs affirmations and negations will be marked for the Book Output Database (BOD) keeping the chronology.\n",
        "# 7.\tAll the paragraphs in the BOD will be split in 9 for CVM’s possibilities (row 2 and 3) – persons and tenses.\n",
        "# 8.\tApply the same processes from Database processing for Book Intent Database.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMDsv5rSby-O"
      },
      "source": [
        "use existing algorithms + algorithms used in the pipelines Input processing I & II / Composed words/ Grammar Semantics.\n",
        "\n",
        "for code examples see pipeline Database processing"
      ]
    }
  ]
}