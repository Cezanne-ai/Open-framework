{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "35. Database training dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qGwEWPhYN3v"
      },
      "source": [
        "This is a complex part in the model because we are dealing with 4 different types of database training (some types of training are not, however, similar with the AI concept of training):\n",
        "\n",
        "•\tPirkin 2 model database training by using experts’ answers (core inputs). Experts in different fields (programmers, AI/NLP engineers, doctors, lawyers, HR specialists, investment advisors, restaurants evaluators…) will provide answers to different possible questions or to advisory/consultancy sessions (which can be addressed by the user in a million possible ways, in short inputs or complex phrases). The bot must identify if the user’s input, summarized in a SPCA sentence (received by the bot in 2 vectors: k-word + embedded vector for each word in the SPCA) has a correspondent in the expert’s answers. For this scope we will need a database with questions/answers or situations/information that can be obtained from a specialized site, from an interview with the expert or even specialized books. The bot cannot interfere in the answer and generate a different/adapted answer (in SPCA answer, slots included in NER and DER will be treated as slots types). The labeling can be done with very simplistic utterances as a pro for the Pirkin 2 model. This will provide a very smooth training process without an important focus on testing that anyway will be in scope of the Cezanne-ai project when we put side by side the results from the 2 models’ implementation (SPCA vs back-up).\n",
        "\n",
        "•\tBack up answer for core inputs is generated by one of the three E2E models. We made the setup in NIU, so there is no need for further clarifications.\n",
        "\n",
        "•\tPirkin 2 model database training of chit chats (in a similar way, we will use databases for reply to feed up the Reaction analysis from CPL). This process of training will be close to the first one, but NER and DER are not an issue.\n",
        "\n",
        "•\tSecondary flow training (chit chats and replies) is an intent-based model that comes first in the NOG flow.\n",
        "\n",
        "All utterances in the database will be trained and we are going to allocate an incremented number (notation) to each correspondent answer to check the duplications in the conversations. \n",
        "\n",
        "It’s important to state that the training process will be ongoing and will be automatically updated - see Auto-training pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OYsRtnjUxYb"
      },
      "source": [
        "# IN: all databases\n",
        "# OUT: k-vectors, embedding vectors, E2E training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_puEdyGZYmXn"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tTraining the database for Pirkin Model;\n",
        "\n",
        "•\tTraining the database for Chitchats, Replies and back up answer;\n",
        "\n",
        "Dependencies: NOG from databases/NIU;\n",
        "\n",
        "Database/ Vocabularies/External needed: all,\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv_Opp11YqNs"
      },
      "source": [
        "# To dos:\n",
        "# Pirkin model\n",
        " # The database with questions will go through NIU pipeline (the same as an input) except: CVM, IVM, AVM, Backup, Secondary, Books processing, Auto Complete, Triggering, Domain Validation, SPCA memory;\n",
        " # Each resulting SPCA (that will have k-vector’s and an embedding vector’s) will be linked with the answer from the databases and chronological indices will be applied.\n",
        "# E2E – DL model\n",
        "  # It will be a standard training included into an E2E model. Also the answers will be marked not to duplicate. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfOkuPO2Y2Jt"
      },
      "source": [
        "see NIU and Chitchat proposed algorithms."
      ]
    }
  ]
}