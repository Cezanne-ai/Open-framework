{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "36. Books training dummy code",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxGiNdkmZkgH"
      },
      "source": [
        "Using books for query purposes can be regarded as a continual learning process of the bot. As the bot is positioned as an expert, what better way to further improve his knowledge base?\n",
        "\n",
        "We will use the Book processing results in terms of paragraphs that are keeping the original chronology and dependence on the book/chapters/subchapters. This process is similar with training databases for Pirkin 2 Model- SPCA, except the labeled answers which will be in scope of Labyrinth model and described more in NOG from books.\n",
        "\n",
        "We will also use an encoder similar to BERT and use book corpuses to fine-tune the language model of the back-up models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2mL-a4qZgPc"
      },
      "source": [
        "# IN: Books processing/ NIU\n",
        "# OUT: trained books for NOG "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Jwf7QZZ7Fn"
      },
      "source": [
        "Objectives:\n",
        "\n",
        "•\tTraining books inputs;\n",
        "\n",
        "•\tUsing Books processing output;\n",
        "\n",
        "•\tMaking summarization using SPCA for all books output database;\n",
        "\n",
        "Dependencies: NOG from books/NIU/ Books processing;\n",
        "\n",
        "Database/ Vocabularies/External needed: Books Input Database, Books Output Database\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omnr77MNaNjq"
      },
      "source": [
        "# To dos:\n",
        "# Inputs:\n",
        " # The Book Input Database will go through NIU – Machine Learning pipelines except: Backup, Secondary, Books processing, Auto Complete, Triggering, Domain Validation, SPCA memory – keep in mind that in Book processing we’ve also used Machine Education;\n",
        " # The resulting SPCAs will be linked with immediate results from the Book Output Database, to establish the chapter/subchapter that will be queried. These results will be linked with the Labyrinth model and the principles from the deep conversational answers.\n",
        " # The chapter will be blocked in the user queries states in order not to exist without considering the exit terms stipulated in the user queries pipeline.  \n",
        "# NOG5 Summarization\n",
        " # The Book Outputs Database will go through the same process as Database training for Pirkin Model.\n",
        " # The resulted SPCA will be kept for Labyrinth model – NOG5 Summarization; the immediate answer will apply,\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hMJp0XsaNhk"
      },
      "source": [
        "# use codes from NIU"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}